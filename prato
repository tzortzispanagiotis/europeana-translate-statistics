Campaign name: crafted-prato
Total records in campaign: 1000
Total annotations: 9434
Correct annotations: 7068
Software annotations: 3693
Software annotations with feedback: 1553
Total upvotes: 20095
Total downvotes: 2319
Human annotations: 3375

Total accuracy statistics for algorithms:
Object Detection Stats:
Total annotations from object detection algorithm: 2140
Total  approved annotations from object detection algorithm: 1959
Object algorithm precision: 0.915
Object algorithm recall: 0.372
Object algorithm accuracy: 0.8

Statistics by color:
{'beige': {'total': 232,
           'upvotes': 1812,
           'downvotes': 16,
           'true_positives': 232,
           'true_negatives': 768,
           'false_positives': 0,
           'false_negatives': 513,
           'precision': 1.0,
           'recall': 0.311,
           'accuracy': 0.661},
 'black': {'total': 100,
           'upvotes': 562,
           'downvotes': 69,
           'true_positives': 88,
           'true_negatives': 912,
           'false_positives': 12,
           'false_negatives': 175,
           'precision': 0.88,
           'recall': 0.335,
           'accuracy': 0.842},
 'blue': {'total': 13,
          'upvotes': 99,
          'downvotes': 20,
          'true_positives': 10,
          'true_negatives': 990,
          'false_positives': 3,
          'false_negatives': 188,
          'precision': 0.769,
          'recall': 0.051,
          'accuracy': 0.84},
 'brown': {'total': 630,
           'upvotes': 2928,
           'downvotes': 485,
           'true_positives': 574,
           'true_negatives': 426,
           'false_positives': 56,
           'false_negatives': 93,
           'precision': 0.911,
           'recall': 0.861,
           'accuracy': 0.87},
 'cyan': {'total': 7,
          'upvotes': 71,
          'downvotes': 1,
          'true_positives': 7,
          'true_negatives': 993,
          'false_positives': 0,
          'false_negatives': 93,
          'precision': 1.0,
          'recall': 0.07,
          'accuracy': 0.915},
 'green': {'total': 232,
           'upvotes': 875,
           'downvotes': 240,
           'true_positives': 176,
           'true_negatives': 824,
           'false_positives': 56,
           'false_negatives': 128,
           'precision': 0.759,
           'recall': 0.579,
           'accuracy': 0.845},
 'grey': {'total': 518,
          'upvotes': 2391,
          'downvotes': 324,
          'true_positives': 480,
          'true_negatives': 520,
          'false_positives': 38,
          'false_negatives': 116,
          'precision': 0.927,
          'recall': 0.805,
          'accuracy': 0.867},
 'olive': {'total': 95,
           'upvotes': 436,
           'downvotes': 71,
           'true_positives': 82,
           'true_negatives': 918,
           'false_positives': 13,
           'false_negatives': 304,
           'precision': 0.863,
           'recall': 0.212,
           'accuracy': 0.759},
 'orange': {'total': 29,
            'upvotes': 284,
            'downvotes': 20,
            'true_positives': 28,
            'true_negatives': 972,
            'false_positives': 1,
            'false_negatives': 389,
            'precision': 0.966,
            'recall': 0.067,
            'accuracy': 0.719},
 'pink': {'total': 18,
          'upvotes': 185,
          'downvotes': 1,
          'true_positives': 18,
          'true_negatives': 982,
          'false_positives': 0,
          'false_negatives': 270,
          'precision': 1.0,
          'recall': 0.062,
          'accuracy': 0.787},
 'purple': {'total': 12,
            'upvotes': 157,
            'downvotes': 1,
            'true_positives': 11,
            'true_negatives': 989,
            'false_positives': 1,
            'false_negatives': 109,
            'precision': 0.917,
            'recall': 0.092,
            'accuracy': 0.901},
 'red': {'total': 163,
         'upvotes': 1405,
         'downvotes': 40,
         'true_positives': 162,
         'true_negatives': 838,
         'false_positives': 1,
         'false_negatives': 224,
         'precision': 0.994,
         'recall': 0.42,
         'accuracy': 0.816},
 'white': {'total': 61,
           'upvotes': 452,
           'downvotes': 11,
           'true_positives': 61,
           'true_negatives': 939,
           'false_positives': 0,
           'false_negatives': 345,
           'precision': 1.0,
           'recall': 0.15,
           'accuracy': 0.743},
 'yellow': {'total': 30,
            'upvotes': 208,
            'downvotes': 0,
            'true_positives': 30,
            'true_negatives': 970,
            'false_positives': 0,
            'false_negatives': 365,
            'precision': 1.0,
            'recall': 0.076,
            'accuracy': 0.733}}

Simple algorithm stats:
Total annotations from simple algorithm: 1553
Total  approved annotations from simple algorithm: 1412
Simple algorithm precision: 0.909
Simple algorithm recall: 0.299
Simple algorithm accuracy: 0.802

Statistics by color:
{'beige': {'total': 148,
           'upvotes': 1098,
           'downvotes': 1,
           'true_positives': 148,
           'true_negatives': 852,
           'false_positives': 0,
           'false_negatives': 513,
           'precision': 1.0,
           'recall': 0.224,
           'accuracy': 0.661},
 'black': {'total': 61,
           'upvotes': 294,
           'downvotes': 49,
           'true_positives': 51,
           'true_negatives': 949,
           'false_positives': 10,
           'false_negatives': 175,
           'precision': 0.836,
           'recall': 0.226,
           'accuracy': 0.844},
 'blue': {'total': 103,
          'upvotes': 510,
          'downvotes': 134,
          'true_positives': 79,
          'true_negatives': 921,
          'false_positives': 24,
          'false_negatives': 188,
          'precision': 0.767,
          'recall': 0.296,
          'accuracy': 0.825},
 'brown': {'total': 504,
           'upvotes': 2526,
           'downvotes': 356,
           'true_positives': 466,
           'true_negatives': 534,
           'false_positives': 38,
           'false_negatives': 93,
           'precision': 0.925,
           'recall': 0.834,
           'accuracy': 0.884},
 'cyan': {'total': 2,
          'upvotes': 19,
          'downvotes': 0,
          'true_positives': 2,
          'true_negatives': 998,
          'false_positives': 0,
          'false_negatives': 93,
          'precision': 1.0,
          'recall': 0.021,
          'accuracy': 0.915},
 'green': {'total': 162,
           'upvotes': 637,
           'downvotes': 153,
           'true_positives': 128,
           'true_negatives': 872,
           'false_positives': 34,
           'false_negatives': 128,
           'precision': 0.79,
           'recall': 0.5,
           'accuracy': 0.861},
 'grey': {'total': 364,
          'upvotes': 1761,
          'downvotes': 206,
          'true_positives': 344,
          'true_negatives': 656,
          'false_positives': 20,
          'false_negatives': 116,
          'precision': 0.945,
          'recall': 0.748,
          'accuracy': 0.88},
 'olive': {'total': 47,
           'upvotes': 206,
           'downvotes': 50,
           'true_positives': 38,
           'true_negatives': 962,
           'false_positives': 9,
           'false_negatives': 304,
           'precision': 0.809,
           'recall': 0.111,
           'accuracy': 0.762},
 'orange': {'total': 17,
            'upvotes': 176,
            'downvotes': 12,
            'true_positives': 16,
            'true_negatives': 984,
            'false_positives': 1,
            'false_negatives': 389,
            'precision': 0.941,
            'recall': 0.04,
            'accuracy': 0.719},
 'pink': {'total': 6,
          'upvotes': 45,
          'downvotes': 1,
          'true_positives': 6,
          'true_negatives': 994,
          'false_positives': 0,
          'false_negatives': 270,
          'precision': 1.0,
          'recall': 0.022,
          'accuracy': 0.787},
 'purple': {'total': 15,
            'upvotes': 163,
            'downvotes': 3,
            'true_positives': 14,
            'true_negatives': 986,
            'false_positives': 1,
            'false_negatives': 109,
            'precision': 0.933,
            'recall': 0.114,
            'accuracy': 0.901},
 'red': {'total': 35,
         'upvotes': 231,
         'downvotes': 17,
         'true_positives': 35,
         'true_negatives': 965,
         'false_positives': 0,
         'false_negatives': 224,
         'precision': 1.0,
         'recall': 0.135,
         'accuracy': 0.817},
 'white': {'total': 65,
           'upvotes': 399,
           'downvotes': 38,
           'true_positives': 61,
           'true_negatives': 939,
           'false_positives': 4,
           'false_negatives': 345,
           'precision': 0.938,
           'recall': 0.15,
           'accuracy': 0.741},
 'yellow': {'total': 24,
            'upvotes': 165,
            'downvotes': 0,
            'true_positives': 24,
            'true_negatives': 976,
            'false_positives': 0,
            'false_negatives': 365,
            'precision': 1.0,
            'recall': 0.062,
            'accuracy': 0.733}}
Human annotation stats:
{'beige': {'total': 517, 'approved': 427},
 'black': {'total': 180, 'approved': 123},
 'blue': {'total': 192, 'approved': 127},
 'brown': {'total': 93, 'approved': 73},
 'cyan': {'total': 96, 'approved': 39},
 'green': {'total': 131, 'approved': 85},
 'grey': {'total': 118, 'approved': 54},
 'olive': {'total': 311, 'approved': 245},
 'orange': {'total': 394, 'approved': 301},
 'pink': {'total': 274, 'approved': 158},
 'purple': {'total': 121, 'approved': 73},
 'red': {'total': 231, 'approved': 174},
 'white': {'total': 347, 'approved': 237},
 'yellow': {'total': 370, 'approved': 281}}
