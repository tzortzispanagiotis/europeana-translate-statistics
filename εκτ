Campaign name: crafted-ekt
Total records in campaign: 2000
Total annotations: 9620
Correct annotations: 8623
Software annotations: 4370
Software annotations with feedback: 4232
Total upvotes: 12536
Total downvotes: 6450
Human annotations: 4253

Total accuracy statistics for algorithms:

Simple algorithm stats:
Total annotations from simple algorithm: 4370
Total  approved annotations from simple algorithm: 2617
Simple algorithm precision: 0.618
Simple algorithm recall: 0.392
Simple algorithm accuracy: 0.832

Statistics by color:
{'beige': {'total': 247,
           'upvotes': 1021,
           'downvotes': 217,
           'true_positives': 202,
           'true_negatives': 1798,
           'false_positives': 44,
           'false_negatives': 554,
           'precision': 0.821,
           'recall': 0.267,
           'accuracy': 0.77},
 'black': {'total': 753,
           'upvotes': 4060,
           'downvotes': 384,
           'true_positives': 680,
           'true_negatives': 1320,
           'false_positives': 71,
           'false_negatives': 374,
           'precision': 0.905,
           'recall': 0.645,
           'accuracy': 0.818},
 'blue': {'total': 129,
          'upvotes': 428,
          'downvotes': 119,
          'true_positives': 98,
          'true_negatives': 1902,
          'false_positives': 28,
          'false_negatives': 380,
          'precision': 0.778,
          'recall': 0.205,
          'accuracy': 0.831},
 'brown': {'total': 647,
           'upvotes': 1383,
           'downvotes': 1073,
           'true_positives': 332,
           'true_negatives': 1668,
           'false_positives': 285,
           'false_negatives': 220,
           'precision': 0.538,
           'recall': 0.601,
           'accuracy': 0.798},
 'cyan': {'total': 21,
          'upvotes': 62,
          'downvotes': 17,
          'true_positives': 14,
          'true_negatives': 1986,
          'false_positives': 6,
          'false_negatives': 74,
          'precision': 0.7,
          'recall': 0.159,
          'accuracy': 0.962},
 'green': {'total': 25,
           'upvotes': 99,
           'downvotes': 16,
           'true_positives': 21,
           'true_negatives': 1979,
           'false_positives': 3,
           'false_negatives': 193,
           'precision': 0.875,
           'recall': 0.098,
           'accuracy': 0.911},
 'grey': {'total': 1686,
          'upvotes': 2711,
          'downvotes': 3622,
          'true_positives': 693,
          'true_negatives': 1307,
          'false_positives': 920,
          'false_negatives': 26,
          'precision': 0.43,
          'recall': 0.964,
          'accuracy': 0.679},
 'olive': {'total': 371,
           'upvotes': 792,
           'downvotes': 629,
           'true_positives': 192,
           'true_negatives': 1808,
           'false_positives': 162,
           'false_negatives': 115,
           'precision': 0.542,
           'recall': 0.625,
           'accuracy': 0.878},
 'orange': {'total': 27,
            'upvotes': 134,
            'downvotes': 14,
            'true_positives': 25,
            'true_negatives': 1975,
            'false_positives': 2,
            'false_negatives': 229,
            'precision': 0.926,
            'recall': 0.098,
            'accuracy': 0.896},
 'pink': {'total': 35,
          'upvotes': 145,
          'downvotes': 12,
          'true_positives': 34,
          'true_negatives': 1966,
          'false_positives': 1,
          'false_negatives': 209,
          'precision': 0.971,
          'recall': 0.14,
          'accuracy': 0.905},
 'purple': {'total': 55,
            'upvotes': 94,
            'downvotes': 119,
            'true_positives': 24,
            'true_negatives': 1976,
            'false_positives': 30,
            'false_negatives': 191,
            'precision': 0.444,
            'recall': 0.112,
            'accuracy': 0.9},
 'red': {'total': 157,
         'upvotes': 735,
         'downvotes': 57,
         'true_positives': 136,
         'true_negatives': 1864,
         'false_positives': 19,
         'false_negatives': 366,
         'precision': 0.877,
         'recall': 0.271,
         'accuracy': 0.839},
 'white': {'total': 205,
           'upvotes': 805,
           'downvotes': 165,
           'true_positives': 153,
           'true_negatives': 1847,
           'false_positives': 45,
           'false_negatives': 638,
           'precision': 0.773,
           'recall': 0.193,
           'accuracy': 0.745},
 'yellow': {'total': 12,
            'upvotes': 67,
            'downvotes': 6,
            'true_positives': 11,
            'true_negatives': 1989,
            'false_positives': 1,
            'false_negatives': 485,
            'precision': 0.917,
            'recall': 0.022,
            'accuracy': 0.805}}
Human annotation stats:
{'beige': {'total': 588, 'approved': 525},
 'black': {'total': 392, 'approved': 346},
 'blue': {'total': 390, 'approved': 357},
 'brown': {'total': 238, 'approved': 200},
 'cyan': {'total': 79, 'approved': 70},
 'green': {'total': 204, 'approved': 182},
 'grey': {'total': 32, 'approved': 24},
 'olive': {'total': 129, 'approved': 104},
 'orange': {'total': 242, 'approved': 212},
 'pink': {'total': 214, 'approved': 201},
 'purple': {'total': 201, 'approved': 172},
 'red': {'total': 382, 'approved': 355},
 'white': {'total': 660, 'approved': 605},
 'yellow': {'total': 502, 'approved': 459}}
