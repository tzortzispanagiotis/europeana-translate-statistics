Campaign name: crafted-momu
Total records in campaign: 1998
Total annotations: 9676
Correct annotations: 8971
Software annotations: 4390
Software annotations with feedback: 3714
Total upvotes: 11404
Total downvotes: 4080
Human annotations: 4581

Total accuracy statistics for algorithms:

Simple algorithm stats:
Total annotations from simple algorithm: 4390
Total  approved annotations from simple algorithm: 2887
Simple algorithm precision: 0.767
Simple algorithm recall: 0.434
Simple algorithm accuracy: 0.859

Statistics by color:
{'beige': {'total': 388,
           'upvotes': 1581,
           'downvotes': 113,
           'true_positives': 336,
           'true_negatives': 1662,
           'false_positives': 15,
           'false_negatives': 589,
           'precision': 0.957,
           'recall': 0.363,
           'accuracy': 0.768},
 'black': {'total': 513,
           'upvotes': 1967,
           'downvotes': 221,
           'true_positives': 419,
           'true_negatives': 1579,
           'false_positives': 50,
           'false_negatives': 396,
           'precision': 0.893,
           'recall': 0.514,
           'accuracy': 0.818},
 'blue': {'total': 90,
          'upvotes': 374,
          'downvotes': 34,
          'true_positives': 74,
          'true_negatives': 1924,
          'false_positives': 7,
          'false_negatives': 297,
          'precision': 0.914,
          'recall': 0.199,
          'accuracy': 0.868},
 'brown': {'total': 512,
           'upvotes': 1060,
           'downvotes': 529,
           'true_positives': 276,
           'true_negatives': 1722,
           'false_positives': 130,
           'false_negatives': 202,
           'precision': 0.68,
           'recall': 0.577,
           'accuracy': 0.858},
 'cyan': {'total': 14,
          'upvotes': 19,
          'downvotes': 13,
          'true_positives': 7,
          'true_negatives': 1991,
          'false_positives': 4,
          'false_negatives': 113,
          'precision': 0.636,
          'recall': 0.058,
          'accuracy': 0.945},
 'green': {'total': 138,
           'upvotes': 212,
           'downvotes': 218,
           'true_positives': 73,
           'true_negatives': 1925,
           'false_positives': 62,
           'false_negatives': 183,
           'precision': 0.541,
           'recall': 0.285,
           'accuracy': 0.891},
 'grey': {'total': 1652,
          'upvotes': 3270,
          'downvotes': 1975,
          'true_positives': 976,
          'true_negatives': 1022,
          'false_positives': 394,
          'false_negatives': 65,
          'precision': 0.712,
          'recall': 0.938,
          'accuracy': 0.813},
 'olive': {'total': 214,
           'upvotes': 358,
           'downvotes': 285,
           'true_positives': 99,
           'true_negatives': 1899,
           'false_positives': 68,
           'false_negatives': 188,
           'precision': 0.593,
           'recall': 0.345,
           'accuracy': 0.886},
 'orange': {'total': 47,
            'upvotes': 121,
            'downvotes': 21,
            'true_positives': 38,
            'true_negatives': 1960,
            'false_positives': 7,
            'false_negatives': 295,
            'precision': 0.844,
            'recall': 0.114,
            'accuracy': 0.869},
 'pink': {'total': 42,
          'upvotes': 149,
          'downvotes': 18,
          'true_positives': 35,
          'true_negatives': 1963,
          'false_positives': 5,
          'false_negatives': 205,
          'precision': 0.875,
          'recall': 0.146,
          'accuracy': 0.905},
 'purple': {'total': 74,
            'upvotes': 84,
            'downvotes': 143,
            'true_positives': 28,
            'true_negatives': 1970,
            'false_positives': 39,
            'false_negatives': 119,
            'precision': 0.418,
            'recall': 0.19,
            'accuracy': 0.927},
 'red': {'total': 122,
         'upvotes': 409,
         'downvotes': 35,
         'true_positives': 102,
         'true_negatives': 1896,
         'false_positives': 10,
         'false_negatives': 264,
         'precision': 0.911,
         'recall': 0.279,
         'accuracy': 0.879},
 'white': {'total': 550,
           'upvotes': 1695,
           'downvotes': 462,
           'true_positives': 357,
           'true_negatives': 1641,
           'false_positives': 72,
           'false_negatives': 463,
           'precision': 0.832,
           'recall': 0.435,
           'accuracy': 0.789},
 'yellow': {'total': 34,
            'upvotes': 105,
            'downvotes': 13,
            'true_positives': 28,
            'true_negatives': 1970,
            'false_positives': 3,
            'false_negatives': 333,
            'precision': 0.903,
            'recall': 0.078,
            'accuracy': 0.856}}
Human annotation stats:
{'beige': {'total': 665, 'approved': 397},
 'black': {'total': 491, 'approved': 268},
 'blue': {'total': 332, 'approved': 190},
 'brown': {'total': 260, 'approved': 131},
 'cyan': {'total': 159, 'approved': 60},
 'green': {'total': 217, 'approved': 113},
 'grey': {'total': 123, 'approved': 41},
 'olive': {'total': 223, 'approved': 114},
 'orange': {'total': 397, 'approved': 177},
 'pink': {'total': 248, 'approved': 114},
 'purple': {'total': 152, 'approved': 50},
 'red': {'total': 320, 'approved': 170},
 'white': {'total': 566, 'approved': 350},
 'yellow': {'total': 428, 'approved': 231}}
