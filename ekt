Campaign name: crafted-ekt
Total records in campaign: 2000
Total annotations: 11081
Correct annotations: 9735
Software annotations: 4370
Software annotations with feedback: 4323
Total upvotes: 15266
Total downvotes: 7985
Human annotations: 5365

Total accuracy statistics for algorithms:

Simple algorithm stats:
Total annotations from simple algorithm: 4370
Total  approved annotations from simple algorithm: 2587
Simple algorithm precision: 0.598
Simple algorithm recall: 0.336
Simple algorithm accuracy: 0.803

Statistics by color:
{'beige': {'total': 247,
           'upvotes': 1255,
           'downvotes': 261,
           'true_positives': 203,
           'true_negatives': 1797,
           'false_positives': 44,
           'false_negatives': 676,
           'precision': 0.822,
           'recall': 0.231,
           'accuracy': 0.735},
 'black': {'total': 753,
           'upvotes': 5115,
           'downvotes': 444,
           'true_positives': 692,
           'true_negatives': 1308,
           'false_positives': 61,
           'false_negatives': 442,
           'precision': 0.919,
           'recall': 0.61,
           'accuracy': 0.799},
 'blue': {'total': 129,
          'upvotes': 532,
          'downvotes': 150,
          'true_positives': 98,
          'true_negatives': 1902,
          'false_positives': 31,
          'false_negatives': 462,
          'precision': 0.76,
          'recall': 0.175,
          'accuracy': 0.802},
 'brown': {'total': 647,
           'upvotes': 1643,
           'downvotes': 1330,
           'true_positives': 327,
           'true_negatives': 1673,
           'false_positives': 305,
           'false_negatives': 276,
           'precision': 0.517,
           'recall': 0.542,
           'accuracy': 0.775},
 'cyan': {'total': 21,
          'upvotes': 79,
          'downvotes': 24,
          'true_positives': 16,
          'true_negatives': 1984,
          'false_positives': 4,
          'false_negatives': 147,
          'precision': 0.8,
          'recall': 0.098,
          'accuracy': 0.93},
 'green': {'total': 25,
           'upvotes': 121,
           'downvotes': 21,
           'true_positives': 21,
           'true_negatives': 1979,
           'false_positives': 4,
           'false_negatives': 264,
           'precision': 0.84,
           'recall': 0.074,
           'accuracy': 0.882},
 'grey': {'total': 1686,
          'upvotes': 3118,
          'downvotes': 4530,
          'true_positives': 654,
          'true_negatives': 1346,
          'false_positives': 1010,
          'false_negatives': 34,
          'precision': 0.393,
          'recall': 0.951,
          'accuracy': 0.657},
 'olive': {'total': 371,
           'upvotes': 924,
           'downvotes': 765,
           'true_positives': 188,
           'true_negatives': 1812,
           'false_positives': 177,
           'false_negatives': 162,
           'precision': 0.515,
           'recall': 0.537,
           'accuracy': 0.855},
 'orange': {'total': 27,
            'upvotes': 169,
            'downvotes': 16,
            'true_positives': 24,
            'true_negatives': 1976,
            'false_positives': 3,
            'false_negatives': 302,
            'precision': 0.889,
            'recall': 0.074,
            'accuracy': 0.868},
 'pink': {'total': 35,
          'upvotes': 175,
          'downvotes': 16,
          'true_positives': 34,
          'true_negatives': 1966,
          'false_positives': 1,
          'false_negatives': 268,
          'precision': 0.971,
          'recall': 0.113,
          'accuracy': 0.881},
 'purple': {'total': 55,
            'upvotes': 114,
            'downvotes': 146,
            'true_positives': 24,
            'true_negatives': 1976,
            'false_positives': 31,
            'false_negatives': 228,
            'precision': 0.436,
            'recall': 0.095,
            'accuracy': 0.885},
 'red': {'total': 157,
         'upvotes': 937,
         'downvotes': 64,
         'true_positives': 138,
         'true_negatives': 1862,
         'false_positives': 19,
         'false_negatives': 442,
         'precision': 0.879,
         'recall': 0.238,
         'accuracy': 0.813},
 'white': {'total': 205,
           'upvotes': 995,
           'downvotes': 210,
           'true_positives': 155,
           'true_negatives': 1845,
           'false_positives': 47,
           'false_negatives': 804,
           'precision': 0.767,
           'recall': 0.162,
           'accuracy': 0.702},
 'yellow': {'total': 12,
            'upvotes': 89,
            'downvotes': 8,
            'true_positives': 11,
            'true_negatives': 1989,
            'false_positives': 1,
            'false_negatives': 608,
            'precision': 0.917,
            'recall': 0.018,
            'accuracy': 0.767}}
Human annotation stats:
{'beige': {'total': 719, 'approved': 482},
 'black': {'total': 468, 'approved': 324},
 'blue': {'total': 475, 'approved': 337},
 'brown': {'total': 295, 'approved': 188},
 'cyan': {'total': 151, 'approved': 67},
 'green': {'total': 274, 'approved': 181},
 'grey': {'total': 43, 'approved': 14},
 'olive': {'total': 180, 'approved': 91},
 'orange': {'total': 322, 'approved': 177},
 'pink': {'total': 276, 'approved': 191},
 'purple': {'total': 246, 'approved': 150},
 'red': {'total': 455, 'approved': 354},
 'white': {'total': 828, 'approved': 602},
 'yellow': {'total': 633, 'approved': 469}}
